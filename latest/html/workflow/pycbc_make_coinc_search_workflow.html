

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>pycbc_make_coinc_search_workflow: A workflow to search for gravitational waves &mdash; PyCBC 0.1 documentation</title>
  

  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  

  

  
    <link rel="top" title="PyCBC 0.1 documentation" href="../index.html"/>
        <link rel="next" title="pycbc_make_sngl_workflow: A single-ifo detchar workflow generator" href="pycbc_make_sngl_workflow.html"/>
        <link rel="prev" title="pycbc_make_psd_estimation_workflow: A workflow generator for noise estimation" href="pycbc_make_psd_estimation_workflow.html"/> 

  
  <script src="../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../index.html" class="icon icon-home"> PyCBC
          

          
          </a>

          
            
            
              <div class="version">
                1.2
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
                <ul>
<li class="toctree-l1"><a class="reference internal" href="../install.html">Installing PyCBC</a></li>
</ul>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="pycbc_make_psd_estimation_workflow.html"><code class="docutils literal"><span class="pre">pycbc_make_psd_estimation_workflow</span></code>: A workflow generator for noise estimation</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href=""><code class="docutils literal"><span class="pre">pycbc_make_coinc_search_workflow</span></code>: A workflow to search for gravitational waves</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="#configuration-file">Configuration file</a></li>
<li class="toctree-l2"><a class="reference internal" href="#generating-the-workflow">Generating the workflow</a></li>
<li class="toctree-l2"><a class="reference internal" href="#planning-and-submitting-the-workflow">Planning and Submitting the Workflow</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#monitor-and-debug-the-workflow-detailed-pegasus-documentation">Monitor and Debug the Workflow (Detailed Pegasus Documentation)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#pegasus-dashboard">Pegasus Dashboard</a></li>
<li class="toctree-l3"><a class="reference internal" href="#pegasus-analyzer">Pegasus Analyzer</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#reuse-of-data-from-a-previous-workflow">Reuse of data from a previous workflow</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#generate-the-full-workflow-you-want-to-do">Generate the full workflow you want to do</a></li>
<li class="toctree-l3"><a class="reference internal" href="#select-the-files-you-want-to-reuse-from-the-prior-run">Select the files you want to reuse from the prior run</a></li>
<li class="toctree-l3"><a class="reference internal" href="#plan-the-workflow">Plan the workflow</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#running-on-the-open-science-grid">Running on the Open Science Grid</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#prerequisites">Prerequisites</a></li>
<li class="toctree-l3"><a class="reference internal" href="#generating-a-cache-file">Generating a cache file</a></li>
<li class="toctree-l3"><a class="reference internal" href="#configuring-the-workflow">Configuring the workflow</a></li>
<li class="toctree-l3"><a class="reference internal" href="#running-the-workflow">Running the workflow</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="pycbc_make_sngl_workflow.html"><code class="docutils literal"><span class="pre">pycbc_make_sngl_workflow</span></code>: A single-ifo detchar workflow generator</a></li>
<li class="toctree-l1"><a class="reference internal" href="pygrb.html"><code class="docutils literal"><span class="pre">pycbc_make_offline_grb_workflow</span></code>: A GRB triggered CBC analysis workflow generator</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../tmpltbank.html">PyCBC template bank generation documentation (<code class="docutils literal"><span class="pre">pycbc.tmpltbank</span></code>)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../hwinj.html">Hardware injection waveform generation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../banksim.html">Calculating the Effectualness (Fitting Factor) of Template Banks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faithsim.html">Dag Generator for Doing Faithfulness Comparisons</a></li>
<li class="toctree-l1"><a class="reference internal" href="../upload_to_gracedb.html">Uploading triggers to gracedb</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../waveform.html">Waveforms</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../documentation.html">Documenting PyCBC code</a></li>
<li class="toctree-l1"><a class="reference internal" href="../release.html">Creating Releases of PyCBC</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../frame.html">Reading Gravitational-wave Frames (<code class="docutils literal"><span class="pre">pycbc.frame</span></code>)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../formats/hdf_format.html">HDF files within the PyCBC workflow</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../workflow.html">Workflow: the inspiral analysis workflow generator (<code class="docutils literal"><span class="pre">pycbc.workflow</span></code>)</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../modules.html">pycbc</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../index.html">PyCBC</a>
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          





<div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../index.html">Docs</a> &raquo;</li>
      
    <li><code class="docutils literal"><span class="pre">pycbc_make_coinc_search_workflow</span></code>: A workflow to search for gravitational waves</li>
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/workflow/pycbc_make_coinc_search_workflow.txt" rel="nofollow"> View page source</a>
          
        
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="pycbc-make-coinc-search-workflow-a-workflow-to-search-for-gravitational-waves">
<h1><code class="docutils literal"><span class="pre">pycbc_make_coinc_search_workflow</span></code>: A workflow to search for gravitational waves<a class="headerlink" href="#pycbc-make-coinc-search-workflow-a-workflow-to-search-for-gravitational-waves" title="Permalink to this headline">¶</a></h1>
<div class="section" id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h2>
<p>The executable <code class="docutils literal"><span class="pre">pycbc_make_coinc_search_workflow</span></code> is a tool used to search
for gravitational waves using data from a network of gravitational-wave
detectors.  It performs all of the necessary steps in the workflow, including
data management, template bank construction (if required), matched filtering
and signal-based vetoes, multi-detector coincidence, data-quality cuts,
background estimation, and software injections to test the search. Its
ultimate task is to determine whether or not a compact binary coalescence is
present in the given data.  The output is a webpage containing the plots that
can be used to understand the results of the analysis</p>
</div>
<div class="section" id="configuration-file">
<span id="configurationfiles"></span><h2>Configuration file<a class="headerlink" href="#configuration-file" title="Permalink to this headline">¶</a></h2>
<p>The behavior of the workflow is controlled by a configuration file (also known as an <code class="docutils literal"><span class="pre">ini</span></code> file) that is made up of three types of sections: workflow, the pegasus profile and the executable options. The workflow sections control how different parts of the the workflow hang together. The pegasus profile sections are equivalent to lines you would have in a condor_submit file (e.g. requirements, storage size etc). Anything you would do in condor you would do here. The third section type maps the options to an executable.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="p">[</span><span class="n">pegasus_profile</span><span class="p">]</span>
<span class="n">condor</span><span class="o">|</span><span class="n">accounting_group</span><span class="o">=</span><span class="n">ligo</span><span class="o">.</span><span class="n">dev</span><span class="o">.</span><span class="n">o1</span><span class="o">.</span><span class="n">cbc</span><span class="o">.</span><span class="n">bns_spin</span><span class="o">.</span><span class="n">pycbcoffline</span>
</pre></div>
</div>
<p>This is used for keeping an account of the pycbc usage</p>
<div class="highlight-python"><div class="highlight"><pre>[workflow]
; https://ldas-jobs.ligo.caltech.edu/~cbc/docs/pycbc/workflow/initialization.html
h1-channel-name = H1:GDS-FAKE_STRAIN
l1-channel-name = L1:GDS-CALIB_STRAIN
file-retention-level = all_triggers
</pre></div>
</div>
<p>You tend to put things in here which will be referred to later (but you can leave it empty). This is a nice way to keep options the same without the need to repeatedly define them. Here the L1/H1 data channel name are given. We also add an option to keep all the triggers/plots the analysis produces</p>
<div class="highlight-python"><div class="highlight"><pre>[workflow-ifos]
l1 =
h1 =
</pre></div>
</div>
<p>Set up which detectors you are going to run over. A blank space after an equals sign denotes True.</p>
<div class="highlight-python"><div class="highlight"><pre>[workflow-datafind]
; See https://ldas-jobs.ligo.caltech.edu/~cbc/docs/pycbc/workflow/datafind.html
datafind-method = AT_RUNTIME_SINGLE_FRAMES
datafind-h1-frame-type = H1_ER_C00_AGG
datafind-l1-frame-type = L1_ER_C01_L1
datafind-check-segment-gaps = update_times
datafind-check-frames-exist = raise_error
datafind-check-segment-summary = no_test
</pre></div>
</div>
<p>This section defines which frames we are going to use and employs different levels of checks to see whether the data exists, there are gaps etc.</p>
<ul class="simple">
<li><code class="docutils literal"><span class="pre">‘datafind-method’</span></code> states how we are going to find the frames. The ‘AT_RUNTIME_SINGLE_FRAMES’ means the executable returns a list of single frame files. You can however provide a cache file, but then the options need to be changed.</li>
<li><code class="docutils literal"><span class="pre">‘datafind-h1-frame-type’</span></code> refers to the frame type the H1 channel name will be found in for the time specified. Same for L1.</li>
<li><code class="docutils literal"><span class="pre">‘datafind-check-segment-gaps’</span></code> option checks to see if there are gaps in the segments from the segment database and the option ‘update_times’ will change the analysis times to skip over these gaps.</li>
<li><code class="docutils literal"><span class="pre">‘datafind-check-frames-exist’</span></code> checks to see if the frames you are looking at actually exists, and if they don’t the ‘raise_error’ option will stop the workflow.</li>
<li><code class="docutils literal"><span class="pre">‘datafind-check-segment-summary’</span></code> Checks the segment summary table and makes sure that the frames exist for all times that the segments are known</li>
</ul>
<div class="highlight-python"><div class="highlight"><pre>[workflow-segments]
; See https://ldas-jobs.ligo.caltech.edu/~cbc/docs/pycbc/workflow/segments.html
segments-method = AT_RUNTIME
segments-h1-science-name = H1:DMT-SCIENCE:1
segments-l1-science-name = L1:DMT-ANALYSIS_READY:1
segments-database-url = https://dqsegdb5.phy.syr.edu
segments-veto-definer-url = https://www.lsc-group.phys.uwm.edu/ligovirgo/cbc/public/segments/ER6/H1L1V1-ER6_GDS_CALIB_STRAIN.xml
segments-science-veto = 1
segments-veto-groups =
segments-final-veto-group = 1
</pre></div>
</div>
<p>This section does a series of checks to the segment database for the segments you need for your analysis.</p>
<ul class="simple">
<li><code class="docutils literal"><span class="pre">‘segments-method’</span></code> option should not change.</li>
<li><code class="docutils literal"><span class="pre">‘segments-h1-science-name’</span></code> option specifies the segment name at LHO we consider to flag science time. The same is given for L1.</li>
<li><code class="docutils literal"><span class="pre">‘segments-data-url’</span></code> specifies the url for the segment database we want to query.</li>
<li><code class="docutils literal"><span class="pre">‘segments-veto-definer-url’</span></code> is the url for the veto definer file we want to use for the search.</li>
<li><code class="docutils literal"><span class="pre">‘segments-science-veto’</span></code> species which category of veto you want to eliminate from your search before it is performed to consider the data science. In this instance, 1 denotes that all the times of Cat 1 vetoes. Time vetoed here is not used in any part of the analysis, and is treated as if it were not collected.</li>
<li><code class="docutils literal"><span class="pre">‘segments-veto-groups’</span></code> is an option you can populate with different veto categories and diagnostic plots will be made after each veto is employed.</li>
<li><code class="docutils literal"><span class="pre">‘segments-final-veto-group’</span></code> is an important option as the vetoes defined here will be used to remove triggers from the search before coincidence is performed. An option of 1 will remove all Cat 1 veto times from the analysis before it is performed. If you want to add cat 2 then the option is 12.</li>
</ul>
<div class="highlight-python"><div class="highlight"><pre>[workflow-tmpltbank]
; See https://ldas-jobs.ligo.caltech.edu/~cbc/docs/pycbc/ahope/template_bank.html
tmpltbank-method=PREGENERATED_BANK
tmpltbank-pregenerated-bank=/home/jveitch/projects/mdc/spin/tmpltbanks/nonspin/BNS_NonSpin_30Hz_earlyaLIGO.xml
</pre></div>
</div>
<p>This section specifies which template bank to use</p>
<ul class="simple">
<li><code class="docutils literal"><span class="pre">’tmpltbank-method’</span></code> option specifies whether you want to use a regenerated bank or to make it on the fly. In O1 we will be us a pregenerated bank.</li>
<li><code class="docutils literal"><span class="pre">‘tmpltbank-pregnerated-bank’</span></code> specifies the location of the xml with the pregenerated bank. Note that this exact location is only valid for SUGAR, and that in general one must provide their own template bank.</li>
</ul>
<div class="highlight-python"><div class="highlight"><pre><span class="p">[</span><span class="n">workflow</span><span class="o">-</span><span class="n">splittable</span><span class="p">]</span>
<span class="n">splittable</span><span class="o">-</span><span class="n">method</span> <span class="o">=</span> <span class="n">IN_WORKFLOW</span>
<span class="n">splittable</span><span class="o">-</span><span class="n">num</span><span class="o">-</span><span class="n">banks</span> <span class="o">=</span> <span class="mi">2</span>
</pre></div>
</div>
<p>This section sets the options for splitting the bank to help with computational costs.</p>
<ul class="simple">
<li><code class="docutils literal"><span class="pre">‘splittable-method’</span></code> tells you the method by which to split the bank, in this instance it is IN_WORKFLOW. If you do not want to split the bank, change this option to NOOP</li>
<li><code class="docutils literal"><span class="pre">‘splittable-num-banks’</span></code> specifies how many banks to split the original bank into.</li>
</ul>
<div class="highlight-python"><div class="highlight"><pre>[workflow-matchedfilter]
; See https://ldas-jobs.ligo.caltech.edu/~cbc/docs/pycbc/ahope/matched_filter.html
matchedfilter-method=WORKFLOW_INDEPENDENT_IFOS
min-analysis-segments = 5
max-analysis-segments = 5
output-type = hdf
</pre></div>
</div>
<p>This section defines how the matched filter is going to be performed. Whether it is going to be independent for each detector, and also how the analysis is actually going to be separated in to chunks given the data available.</p>
<ul class="simple">
<li><code class="docutils literal"><span class="pre">‘matched-filter-method’</span></code> defines where the data is going to be separated and searched over, in this instance the data for each IFO will be considered independently and in the workflow</li>
<li><code class="docutils literal"><span class="pre">‘min-analysis-segments’</span></code> defines the minimum number of overlapping chunks you separate the data in to to analyze. This is a proxy for segment length. In this instance 5 has been stated. Therefore if the data cannot be split in to 5 overlapping chunks the code skips over the data. To understand how much time this is you need to look in the [inspiral] options and consider the segment-length and padding options specified. ‘max-analysis-segments’ is the same but for the maximum number of overlapping chunks. Be aware if you lower/raise either of these numbers you will affect the psd estimation.</li>
<li><code class="docutils literal"><span class="pre">‘output-type’</span></code> is the format of the output trigger files from the matched filter search</li>
</ul>
<div class="highlight-python"><div class="highlight"><pre>[workflow-coincidence]
; See https://ldas-jobs.ligo.caltech.edu/~cbc/docs/pycbc/ahope/coincidence.html
parallelization-factor = 10
</pre></div>
</div>
<p>This part of the workflow looks for coincidence between templates between detectors. All coincidences are kept. If you have a large template bank you probably want make the <code class="docutils literal"><span class="pre">‘parallelization-factor’</span></code> large</p>
<div class="highlight-python"><div class="highlight"><pre><span class="p">[</span><span class="n">workflow</span><span class="o">-</span><span class="n">injections</span><span class="p">]</span>
<span class="n">injections</span><span class="o">-</span><span class="n">method</span><span class="o">=</span><span class="n">IN_WORKFLOW</span>
</pre></div>
</div>
<p>This section deals with software injections. Here you are specifying whether to use either pregenerated injections sets or ones made within the workflow itself. In this case, we will use one that is created within the workflow.</p>
<div class="highlight-python"><div class="highlight"><pre>[executables]
; setup of condor universe and location of executables
inspiral          = ${which:pycbc_inspiral}
injections = ${which:lalapps_inspinj}
splittable = ${which:pycbc_splitbank}
segment_query = ${which:ligolw_segment_query_dqsegdb}
segments_from_cats = ${which:ligolw_segments_from_cats_dqsegdb}
llwadd = ${which:ligolw_add}
ligolw_combine_segments = ${which:ligolw_combine_segments}
bank2hdf = ${which:pycbc_coinc_bank2hdf}
hdfinjfind = ${which:pycbc_coinc_hdfinjfind}
coinc = ${which:pycbc_coinc_findtrigs}
statmap = ${which:pycbc_coinc_statmap}
statmap_inj = ${which:pycbc_coinc_statmap_inj}
plot_sensitivity = ${which:pycbc_page_sensitivity}
plot_foundmissed = ${which:pycbc_page_foundmissed}
plot_snrifar = ${which:pycbc_page_snrifar}
page_foreground = ${which:pycbc_page_foreground}
page_injections = ${which:pycbc_page_injtable}
hdf_trigger_merge = ${which:pycbc_coinc_mergetrigs}
plot_snrchi = ${which:pycbc_page_snrchi}
plot_coinc_snrchi = ${which:pycbc_page_coinc_snrchi}
plot_segments = ${which:pycbc_page_segments}
results_page = ${which:pycbc_make_html_page}
</pre></div>
</div>
<p>This section defines where each of the executables live; it tells the workflow which files to process. It might be worth checking you can find all of these paths before you set the code running.</p>
<p>The following options are those associated to a given executable.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="p">[</span><span class="n">llwadd</span><span class="p">]</span>
<span class="p">[</span><span class="n">datafind</span><span class="p">]</span>
<span class="n">urltype</span><span class="o">=</span><span class="nb">file</span>
</pre></div>
</div>
<p>This is the format for the return of the data find executable - you want a file.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="p">[</span><span class="n">segments_from_cats</span><span class="p">]</span>
</pre></div>
</div>
<p>Some sections are left empty. That is fine, but you have to define each option otherwise the code will complain</p>
<div class="highlight-python"><div class="highlight"><pre>[ligolw_combine_segments]

[splittable]
; options for splittable job
random-sort =
</pre></div>
</div>
<p>This option randomly sorts the bank to be split up before processing</p>
<div class="highlight-python"><div class="highlight"><pre><span class="p">[</span><span class="n">injections</span><span class="p">]</span>
<span class="n">waveform</span> <span class="o">=</span> <span class="n">SpinTaylorT4threePointFivePN</span>
</pre></div>
</div>
<p>Define the waveforms you want to use for injections</p>
<div class="highlight-python"><div class="highlight"><pre>[injections-bnslininj]
f-lower = 20
min-distance = 1000
max-distance = 150000
d-distr = uniform
l-distr = random
i-distr = uniform
min-mass1 = 1.0
max-mass1 = 3.1
min-mass2 = 1.0
max-mass2 = 3.1
m-distr = componentMass
min-mtotal = 2.0
max-mtotal = 6.2
disable-spin =
time-step = 89.155
time-interval = 10
seed = 1234
</pre></div>
</div>
<p>These are the injections parameters you want to define. Only defining ones which aren’t so obvious</p>
<ul class="simple">
<li><code class="docutils literal"><span class="pre">f-lower</span></code> = low frequency cut off</li>
<li><code class="docutils literal"><span class="pre">min-distance</span></code> =  (kpc)</li>
<li><code class="docutils literal"><span class="pre">max-distance</span></code> = (kpc)</li>
<li><code class="docutils literal"><span class="pre">d-distr</span></code> = the distance distribution of the injections</li>
<li><code class="docutils literal"><span class="pre">l-distr</span></code> = the distribution of injections in the sky</li>
<li><code class="docutils literal"><span class="pre">i-distr</span></code> = inclination of the injection</li>
<li><code class="docutils literal"><span class="pre">time-step</span></code> = time between injections. This can be whatever time you want, but remember if the injections are too close together you can screw up your psd estimation. ~90s seems ok.</li>
<li><code class="docutils literal"><span class="pre">time-interval</span></code> = time interval to inject the signal. It will not always be exactly at time-step, but at a time of time-step +/- random_number(0,time-interval)</li>
<li><code class="docutils literal"><span class="pre">seed</span></code> = random seed, choose different numbers to get different realizations of the same background distribution</li>
</ul>
<div class="highlight-python"><div class="highlight"><pre>[inspiral]
; inspiral analysis parameters -- added to all inspiral jobs
chisq-bins = 256
snr-threshold = 5.0
approximant = SPAtmplt
order = 7
cluster-method = window
cluster-window = 1.0
segment-length = 512
segment-start-pad = 64
segment-end-pad = 16
psd-estimation = median
psd-segment-length = 16
psd-segment-stride = 8
psd-inverse-length = 16
strain-high-pass = 30
pad-data = 8
processing-scheme = mkl
sample-rate = 4096
filter-inj-only =
low-frequency-cutoff = 40
</pre></div>
</div>
<p>These are the parameters you want to define for the inspiral search</p>
<ul class="simple">
<li><code class="docutils literal"><span class="pre">chisq-bins</span></code> = number of chisq bins for the standard Bruce Allen chisq</li>
<li><code class="docutils literal"><span class="pre">snr-threshold</span></code> = SNR threshold</li>
<li><code class="docutils literal"><span class="pre">approximant</span></code> = approximation you want to use. SPAtmplt is stationary phase approximation template which is a fast implementation of Taylor F2.</li>
<li><code class="docutils literal"><span class="pre">order</span></code> = PN order, the numbers are double the order. So 7=3.5PN</li>
<li><code class="docutils literal"><span class="pre">cluster-method</span></code> = method over which to identify the loudest trigger - in this case a window</li>
<li><code class="docutils literal"><span class="pre">cluster-window</span></code> = take a 1 second window around the loudest trigger</li>
<li><code class="docutils literal"><span class="pre">segment-length</span></code> = the length of a segment you want to analyze. Remember previously we mention we want 5 overlapping segments</li>
<li><code class="docutils literal"><span class="pre">segment-start-pad</span></code> = the amount of time we want to pad the start of the data by. In this instance we want to not use the first 64 seconds of data, as it will contain errors from filtering. This takes in to account the length of time we lose due to PSD corruption (16s) and the wrap around effect we have due to the template (48s)</li>
<li><code class="docutils literal"><span class="pre">segment-end-pad</span></code> = the amount of time we want to pad the end of the data by. See above.</li>
<li><code class="docutils literal"><span class="pre">psd-estimation</span></code> = the method by which we want to estimate the psd</li>
<li><code class="docutils literal"><span class="pre">psd-segment-length</span></code> = length of time used in each psd calculation</li>
<li><code class="docutils literal"><span class="pre">psd-segment-stride</span></code> = time spacing between each psd calculation. 16s length with 8s stride implies a 50% overlap</li>
<li><code class="docutils literal"><span class="pre">psd-inverse-length</span></code> = time length used to truncate the inverse FFT (that is, the time domain realization) of the psd</li>
<li><code class="docutils literal"><span class="pre">strain-high-pass</span></code> = high pass filter applied to strain data before psd estimation</li>
<li><code class="docutils literal"><span class="pre">pad-data</span></code> = 8 second padding added to beginning of data to account for filter corruption for resampling and high-pass before data is broken up into chunks</li>
<li><code class="docutils literal"><span class="pre">processing-scheme</span></code> = indicates which software to use for processing (MKL = math kernel library made by Intel)</li>
<li><code class="docutils literal"><span class="pre">sample-rate</span></code> = sample rate of data (will be down sampled in workflow)</li>
<li><code class="docutils literal"><span class="pre">filter-inj-only</span></code> = Use only segments with injections in them for matched filter</li>
<li><code class="docutils literal"><span class="pre">low-frequency-cutoff</span></code> = low frequency limit for the matched filter search</li>
</ul>
<div class="highlight-python"><div class="highlight"><pre>[inspiral-h1]
; h1 specific inspiral parameters
channel-name = ${workflow|h1-channel-name}
</pre></div>
</div>
<p>Specify the name of the channel you want to run the inspiral analysis over for H1. Here we are referring back to the name in the workflow module</p>
<div class="highlight-python"><div class="highlight"><pre>[inspiral-l1]
; l1 specific inspiral parameters
channel-name = ${workflow|l1-channel-name}

[bank2hdf]
[trig2hdf]

[coinc]
coinc-threshold = 0.000
</pre></div>
</div>
<p>Here we are doing exact match coincidence. So we take the light travel time between detectors and look for triggers which are coincident within this time window. The threshold defines if you want to extend the window.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="p">[</span><span class="n">coinc</span><span class="o">-</span><span class="n">full</span><span class="p">]</span>
<span class="n">decimation</span><span class="o">-</span><span class="n">factor</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">loudest</span><span class="o">-</span><span class="n">keep</span> <span class="o">=</span> <span class="mi">200</span>
<span class="n">timeslide</span><span class="o">-</span><span class="n">interval</span><span class="o">=</span><span class="mf">1.1</span>
</pre></div>
</div>
<p>This section concerns time slides without injections, and its purpose is to keep a small number of timesmlide triggers for background estimation. Time slides are done at all relative offsets that are multiple of the &#8216;timeslide-interval&#8217;, which is defined here to be 1.1 seconds. We don’t store all the coincident triggers due from time slides. We keep 200 of the loudest triggers from each template time slide, given by the second option, which gives a good estimation of the background at low FAR. The top option specifies for which timeslides we will keep all triggers, to get an overall estimation of background (not just the loudest). In this instance we would keep the triggers from 1000th, 2000th, 3000th timeslide.</p>
<div class="highlight-python"><div class="highlight"><pre>[coinc-injfull&amp;coinc-fullinj]
timeslide-interval={coinc-full:timeslide-interval}
loudest-keep-value = 8.5
cluster-window = {statmap|cluster-window}
</pre></div>
</div>
<p>This section concerns time slides with injections in the data. We assume only one injection will be coincident with a timeslide (done every 1.1 seconds - see first option) trigger and we keep its coincidence if its ranking statistic (newSNR) &gt; 8.5 as specified in the second option. This is to limit storage of unimpactful triggers only.</p>
<div class="highlight-python"><div class="highlight"><pre>[coinc-injinj]

[pegasus_profile-statmap&amp;pegasus_profile-statmap_inj]
condor|request_memory = 20GB
</pre></div>
</div>
<p>This is the amount of memory the jobs might take</p>
<div class="highlight-python"><div class="highlight"><pre><span class="p">[</span><span class="n">statmap</span><span class="o">&amp;</span><span class="n">statmap_inj</span><span class="p">]</span>
<span class="n">veto</span><span class="o">-</span><span class="n">window</span> <span class="o">=</span> <span class="mf">0.050</span>
<span class="n">cluster</span><span class="o">-</span><span class="n">window</span> <span class="o">=</span> <span class="mf">10.0</span>
</pre></div>
</div>
<p>This controls the final clustering after all coincidence testing. The <code class="docutils literal"><span class="pre">cluster-window</span></code> indicates the time window used for clustering.
The <code class="docutils literal"><span class="pre">veto-window</span></code> is used to remove all coincident zero-lag triggers so that they aren&#8217;t included in background estimation</p>
<div class="highlight-python"><div class="highlight"><pre><span class="p">[</span><span class="n">hdfinjfind</span><span class="p">]</span>
<span class="n">injection</span><span class="o">-</span><span class="n">window</span> <span class="o">=</span> <span class="mf">1.0</span>
</pre></div>
</div>
<p>The rest of the config file concerns plotting formats</p>
<div class="highlight-python"><div class="highlight"><pre>[page_foreground]
[plot_snrifar]

[plot_snrchi]
[plot_coinc_snrchi]
[plot_coinc_snrchi-inj]
[plot_coinc_snrchi-bkg]
background-front=
[plot_coinc_snrchi-inj&amp;plot_coinc_snrchi-bkg&amp;plot_snrchi]
newsnr-contours =  6 8 10

[plot_sensitivity]
sig-type = ifar
sig-bins = 1 3 10 30 100 300 1000 3000 10000 30000 100000

[plot_sensitivity-mchirp]
bin-type =  mchirp
bins = 0.89 1.31 1.74 2.17 2.60
min-dist = 40
max-dist = 120
dist-bins = 50

[plot_sensitivity-mtotal]
bin-type =  total_mass
bins = 2 2.4 3.2 4 6
min-dist = 40
max-dist = 120
dist-bins = 50

[plot_sensitivity-spin]
bin-type =  spin
bins = -0.4 -0.2 0.2 0.4
min-dist = 40
max-dist = 120
dist-bins = 50

[plot_sensitivity-mchirp_binless]
bin-type =  mchirp
bins = 0.89 1.31 1.74 2.17 2.60
min-dist = 40
max-dist = 120

[plot_sensitivity-mtotal_binless]
bin-type =  total_mass
bins = 2 2.4 3.2 4 6
min-dist = 40
max-dist = 120

[plot_sensitivity-spin_binless]
bin-type =  spin
bins = -0.4 -0.2 0.2 0.4
min-dist = 40
max-dist = 120

[plot_foundmissed]
[plot_foundmissed-mchirp]
axis-type=mchirp
dynamic=
[plot_foundmissed-chirpdistmchirp]
axis-type=mchirp
dynamic=
distance-type=chirp_distance
[plot_foundmissed-time]
axis-type=time
dynamic=

[plot_foundmissed-mchirp_static]
axis-type=mchirp
log-distance=
[plot_foundmissed-chirpdistmchirp_static]
axis-type=mchirp
distance-type=chirp_distance
log-distance=
[plot_foundmissed-time_static]
axis-type=time
log-distance=

[hdf_trigger_merge]
[pegasus_profile-hdf_trigger_merge]
condor|request_memory = 10GB

[page_injections]
[plot_segments]

[results_page]
analysis-title=&quot;PyCBC Coincident Analysis&quot;
analysis-subtitle=&quot;...&quot;
</pre></div>
</div>
</div>
<div class="section" id="generating-the-workflow">
<span id="coincworkflowgenerate"></span><h2>Generating the workflow<a class="headerlink" href="#generating-the-workflow" title="Permalink to this headline">¶</a></h2>
<p>The workflow is generated by running the script <code class="docutils literal"><span class="pre">pycbc_make_coinc_search_workflow</span></code>. This program takes the command line arguments</p>
<div class="highlight-text"><div class="highlight"><pre>$ pycbc_make_coinc_search_workflow --help
Traceback (most recent call last):
  File &quot;/home/cbc/pycbc/pycbc-virtualenv/bin/pycbc_make_coinc_search_workflow&quot;, line 4, in &lt;module&gt;
    __import__(&#39;pkg_resources&#39;).run_script(&#39;PyCBC===e8adc5&#39;, &#39;pycbc_make_coinc_search_workflow&#39;)
  File &quot;/home/cbc/pycbc/pycbc-virtualenv/lib/python2.6/site-packages/pkg_resources/__init__.py&quot;, line 735, in run_script
    self.require(requires)[0].run_script(script_name, ns)
  File &quot;/home/cbc/pycbc/pycbc-virtualenv/lib/python2.6/site-packages/pkg_resources/__init__.py&quot;, line 1652, in run_script
    exec(code, namespace, namespace)
  File &quot;/home/cbc/pycbc/pycbc-virtualenv/lib/python2.6/site-packages/PyCBC-e8adc5-py2.6.egg/EGG-INFO/scripts/pycbc_make_coinc_search_workflow&quot;, line 31, in &lt;module&gt;
    import pycbc.events, pycbc.workflow as wf
  File &quot;/home/cbc/pycbc/pycbc-virtualenv/lib/python2.6/site-packages/PyCBC-e8adc5-py2.6.egg/pycbc/workflow/__init__.py&quot;, line 38, in &lt;module&gt;
    from pycbc.workflow.datafind import *
  File &quot;/home/cbc/pycbc/pycbc-virtualenv/lib/python2.6/site-packages/PyCBC-e8adc5-py2.6.egg/pycbc/workflow/datafind.py&quot;, line 38, in &lt;module&gt;
    from pycbc.frame import datafind_connection
  File &quot;/home/cbc/pycbc/pycbc-virtualenv/lib/python2.6/site-packages/PyCBC-e8adc5-py2.6.egg/pycbc/frame.py&quot;, line 19, in &lt;module&gt;
    import lalframe, logging
  File &quot;/home/cbc/pycbc/pycbc-virtualenv/opt/lalsuite/lib64/python2.6/site-packages/lalframe/__init__.py&quot;, line 2, in &lt;module&gt;
    from lalframe import *
  File &quot;/home/cbc/pycbc/pycbc-virtualenv/opt/lalsuite/lib64/python2.6/site-packages/lalframe/lalframe.py&quot;, line 29, in &lt;module&gt;
    _lalframe = swig_import_helper()
  File &quot;/home/cbc/pycbc/pycbc-virtualenv/opt/lalsuite/lib64/python2.6/site-packages/lalframe/lalframe.py&quot;, line 25, in swig_import_helper
    _mod = imp.load_module(&#39;_lalframe&#39;, fp, pathname, description)
ImportError: libframecpp.so.5: cannot open shared object file: No such file or directory
</pre></div>
</div>
<p>The configuration files can either be passes as local files, or given as URLs
to specific configuration files managed for an analysis. For example, to
generate a workflow to search two weeks of S6D data and place the results in
your <code class="docutils literal"><span class="pre">public_html</span></code> directory, run the command:</p>
<div class="highlight-python"><div class="highlight"><pre>pycbc_make_hdf_coinc_workflow --workflow-name s6d_chunk3 --output-dir output \
  --config-files https://code.pycbc.phy.syr.edu/ligo-cbc/pycbc-config/download/master/S6/pipeline/s6_run_pycbc_er8_pre_release.ini \
  https://code.pycbc.phy.syr.edu/ligo-cbc/pycbc-config/download/master/S6/pipeline/executables.ini \
  https://code.pycbc.phy.syr.edu/ligo-cbc/pycbc-config/download/master/S6/pipeline/injections.ini \
  https://code.pycbc.phy.syr.edu/ligo-cbc/pycbc-config/download/master/S6/pipeline/data_S6.ini \
  https://code.pycbc.phy.syr.edu/ligo-cbc/pycbc-config/download/master/S6/pipeline/gps_times_s6d_big_dog_two_weeks.ini \
  --config-overrides &quot;results_page:output-path:${HOME}/public_html/s6/s6d-big-dog-weeks&quot;
</pre></div>
</div>
<p>The configuration <code class="docutils literal"><span class="pre">results_page:output-path</span></code> can be changed appropriately to
set the output web page location.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">To use released exectutables for production analysis, you should specify
the URL to an <code class="docutils literal"><span class="pre">executables.ini</span></code> file from the
<a class="reference external" href="https://code.pycbc.phy.syr.edu/ligo-cbc/pycbc-software">PyCBC Software repository</a>.</p>
</div>
</div>
<div class="section" id="planning-and-submitting-the-workflow">
<span id="coincworkflowplan"></span><h2>Planning and Submitting the Workflow<a class="headerlink" href="#planning-and-submitting-the-workflow" title="Permalink to this headline">¶</a></h2>
<p>Pegasus is used to plan and submit the workflow. To involve Pegasus to plan a
PyCBC workflow, you use the command <code class="docutils literal"><span class="pre">pycbc_submit_dax</span></code> which takes the
command line arguments</p>
<div class="highlight-text"><div class="highlight"><pre>$ pycbc_submit_dax --help
usage: pycbc_submit_dax [-h] --dax DAX [optional arguments]

required arguments:
  -d, --dax DAX           name of the dax file to plan

workflow submission required one of:
  -a, --accounting-group GROUP tagged string used for site 
                               resource accounting.
  -F, --force-no-accounting-group submit without an accounting
                               group. Will cause condor submission
                               to fail on LIGO Data Grid clusters

optional arguments:
  -h, --help              show this help message and exit
  -c, --cache-file FILE   replica cache file for data reuse
  -g, --local-gsiftp-server HOST provide a gsiftp url on HOST
                                 for the local site storage
                                 and scratch directories
  -r, --remote-staging-server HOST provide a gsiftp url on HOST
                                 for the remote site storage
                                 and scratch directories
  -p, --pegasus-properties FILE use the specified file as
                               the pegasus properties file
  -P, --append-pegasus-property STRING add the extra property
                                          specified by the argument
  -s, --execution-sites A,B,C specify a comma separated list
                               of execution sites that will be
                               used in addition to the local site
  -S, --staging-sites A=X,B=Y,C=Z  comma separated list of key=value
                                   pairs, where the key is the
                                   execution site and value is the
                                   staging site for that execution site
  -k, --append-site-profile SITE:NAMESPACE|KEY:VALUE
                               append the profile determined by
                               NAMESPACE, KEY, and VALUE to the
                               site catalog entry for SITE
  -t, --transformation-catalog FILE pass the specified
                                   transformation catalog to Pegasus
  -k, --no-create-proxy   Do not run ligo-proxy-init and assume
                             that the user has a valid grid proxy
  -n, --no-submit         Plan the DAX but do not submit it
  -l, --local-dir         Directory to put condor files under

If the environment variable TMPDIR is set then this is prepended to the 
path to the temporary workflow execte directory passed to pegasus-plan.
If the --local-dir option is not given.

If the environment variable PEGASUS_FILE_DIRECTORY is set then the
script will look there for pegasus site catalog and configuration
otherwise, the script will look for this directory by querying the
pycbc.workflow module.

If the environment variable PEGASUS_SITE_CATALOG_PATH is set then the
script will look for site catalog templates in this directory.
Site catalog templates may contain other environment variables that
must be set in order for them to be rendered correctly.
</pre></div>
</div>
<p>Note that  you are running on a resource that mandates accounting, then you
will also need to add a valid tag with the <code class="docutils literal"><span class="pre">--accounting-tag</span></code> command line
argument. Please see
<a class="reference external" href="https://ldas-gridmon.ligo.caltech.edu/ldg_accounting/user">the LDG accounting page</a>. to
determine the correct tags. These can be applied by adding the following line
to your submit invocation.</p>
<p>For example, to plan and submit the workflow in the example above, change to the directory that you specified with the <code class="docutils literal"><span class="pre">--output</span></code>
command line option to <code class="docutils literal"><span class="pre">pycbc_make_hdf_coinc_workflow</span></code> and plan and submit
the workflow:</p>
<div class="highlight-python"><div class="highlight"><pre>cd output
pycbc_submit_dax --accounting-group ligo.dev.o1.cbc.explore.test --dax s6d_chunk3.dax
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">The above example uses the accounting tag <code class="docutils literal"><span class="pre">ligo.dev.o1.cbc.explore.test</span></code>
which should not be used in practice.</p>
</div>
<p>You can monitor the status of the workflow with Pegasus Dashboard, or the
other Pegasus tools described below.</p>
<p>If the workflow runs successfully, the output will be place under the
directory specified by <code class="docutils literal"><span class="pre">results_page:output-path</span></code> when the workflow is
complete.</p>
<div class="section" id="monitor-and-debug-the-workflow-detailed-pegasus-documentation">
<h3>Monitor and Debug the Workflow (<a class="reference external" href="https://pegasus.isi.edu/wms/docs/latest/tutorial.php#idm78622034400">Detailed Pegasus Documentation</a>)<a class="headerlink" href="#monitor-and-debug-the-workflow-detailed-pegasus-documentation" title="Permalink to this headline">¶</a></h3>
<p>To monitor the above workflow, one would run:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">pegasus</span><span class="o">-</span><span class="n">status</span> <span class="o">/</span><span class="n">usr1</span><span class="o">/</span><span class="n">ahnitz</span><span class="o">/</span><span class="n">log</span><span class="o">/</span><span class="n">ahnitz</span><span class="o">/</span><span class="n">pegasus</span><span class="o">/</span><span class="n">weekly_ahope</span><span class="o">/</span><span class="n">run0011</span>
</pre></div>
</div>
<p>To get debugging information in the case of failures.:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">pegasus</span><span class="o">-</span><span class="n">analyzer</span> <span class="o">/</span><span class="n">usr1</span><span class="o">/</span><span class="n">ahnitz</span><span class="o">/</span><span class="n">log</span><span class="o">/</span><span class="n">ahnitz</span><span class="o">/</span><span class="n">pegasus</span><span class="o">/</span><span class="n">weekly_ahope</span><span class="o">/</span><span class="n">run0011</span>
</pre></div>
</div>
</div>
<div class="section" id="pegasus-dashboard">
<h3>Pegasus Dashboard<a class="headerlink" href="#pegasus-dashboard" title="Permalink to this headline">¶</a></h3>
<p>The <a class="reference external" href="http://pegasus.isi.edu/wms/docs/latest/ch02s11.php">pegasus dashboard</a> is a visual and interactive way to get information about the progress, status, etc of your workflows.</p>
<p>The software can be obtained from a separate pegasus package here &lt;<a class="reference external" href="https://github.com/pegasus-isi/pegasus-service">https://github.com/pegasus-isi/pegasus-service</a>&gt;.</p>
<p>Pegasus Dashboard is currently installed on sugar. To view your Pegasus Dashboard, in a browser go to:</p>
<div class="highlight-python"><div class="highlight"><pre>https://sugar.phy.syr.edu/pegasus/u/albert.einstein
</pre></div>
</div>
<p>This shows a page that has a table of all your workflows that were submitted from sugar. You can view the details of a workflow by clicking on the link in the Workflow Details column of the table.</p>
<p>Clicking on the Workflow Details link will take you to a webpage that gives a high-level overview of the workflow, telling you how many many jobs succeeded, fail, the submit directory, etc. There is a table with tabs at the bottom of the page. If you click the tabs Failed, Running, and Successful the page will generate a table that lists all the failed, running, and successful jobs in the workflow respectively. You also have the ability to search the table for a particular kind of job using the Search bar.</p>
<p>You can view the details of a job by clicking the link in the Job Name column. This will take you to a Job Details page. This page will tell you where to find stdout files, stderr files, how much wall clock time the job took to run, etc. There is a table at the bottom of the page with a Failed and Successful tab. If you click on the respective tab, it will list all invocations of that job. You can click on the link in the Invocations column for more information.</p>
<p>On the Invocation Details page there is information about the command line arguments, executable path, CPU time, wall clock time, etc.</p>
<p>In certain cases, the pegasus monitor daemon may crash and this could result in
invalid or nonsensical information on the dashboard (e.g. a cumulative
computing time of None). This problem can be solved by running
<code class="docutils literal"><span class="pre">pegasus-plots</span></code> on the workflow directory: the command should tell you what
to do. Typically this will be running <code class="docutils literal"><span class="pre">pegasus-monitord</span></code> in replay mode (see
its man page).</p>
</div>
<div class="section" id="pegasus-analyzer">
<h3>Pegasus Analyzer<a class="headerlink" href="#pegasus-analyzer" title="Permalink to this headline">¶</a></h3>
<p>The <a class="reference external" href="http://pegasus.isi.edu/wms/docs/trunk/cli-pegasus-analyzer.php">pegasus analyzer</a> is a command-line tool for reporting sucessful and failed jobs.</p>
<p>To run <code class="docutils literal"><span class="pre">pegasus_analyzer</span></code> on your workflow, type:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">pegasus</span><span class="o">-</span><span class="n">analyzer</span> <span class="o">/</span><span class="n">usr1</span><span class="o">/</span><span class="n">ahnitz</span><span class="o">/</span><span class="n">log</span><span class="o">/</span><span class="n">ahnitz</span><span class="o">/</span><span class="n">pegasus</span><span class="o">/</span><span class="n">weekly_ahope</span><span class="o">/</span><span class="n">run0011</span>
</pre></div>
</div>
<p><code class="docutils literal"><span class="pre">pegasus_analyzer</span></code> will display a summary of suceeded, failed, and unsubmitted jobs in the workflow. After the summary information, <code class="docutils literal"><span class="pre">pegasus_analyzer</span></code> will display information about each failed job. An example would be:</p>
<div class="highlight-python"><div class="highlight"><pre>************************************Summary*************************************

Submit Directory   : /usr1/cbiwer/log/H1L1V1-s6d_test-970012743-258000.9apn7X
Total jobs         :     24 (100.00%)
# jobs succeeded   :     19 (79.17%)
# jobs failed      :      5 (20.83%)
# jobs unsubmitted :      0 (0.00%)

******************************Failed jobs&#39; details******************************

=====================ligolw_cbc_hardware_inj_page_ID000020======================

last state: POST_SCRIPT_FAILED
     site: local
submit file: ligolw_cbc_hardware_inj_page_ID000020.sub
output file: ligolw_cbc_hardware_inj_page_ID000020.out.001
error file: ligolw_cbc_hardware_inj_page_ID000020.err.001

-------------------------------Task #1 - Summary--------------------------------

site        : local
hostname    : avhe2010.sugar.phy.syr.edu
executable  : /home/cbiwer/projects/test_workflow/970012743-970270743/executables/ligolw_cbc_hardware_inj_page
arguments   : --source-xml hardware_injection_summary/H1L1V1-S6_CBC_HW_INJECTIONS-930493015-42111800.xml --outfile hardware_injection_summary/H1L1V1-HWINJ_SUMMARY_CAT_2-9
70012743-258000.html ----segments-xml-glob ../segments/*-SCIENCE_SEGMENTS-*-*.xml --v1-injections ----vetos-xml-glob ../segments/*-COMBINED_CAT_2_VETO_SEGS-*-*.xml --gps-
start-time 970012743 --segment-dir hardware_injection_summary --gps-end-time 970270743 --l1-injections --analyze-injections --cache-file full_data/H1L1V1-INSPIRAL_HIPE_FU
LL_DATA_CAT_2_VETO-970012743-258000.cache --h1-injections --cache-pattern *SIRE_FIRST*
exitcode    : 2
working dir : /home/cbiwer/projects/test_workflow/970012743-970270743

Task #1 - ligo-hwinjpagejob::ligolw_cbc_hardware_inj_page:1.0 - ID000020 - Kickstart stderr

Usage:  ligolw_cbc_hardware_inj_page [options]
Program to parse the inspiral injection log
ligolw_cbc_hardware_inj_page: error: no such option: ----segments-xml-glob
</pre></div>
</div>
<p>The output provides you with the <code class="docutils literal"><span class="pre">stderr</span></code>, the command line, and where the job was run.</p>
<p>If you have a subdax that failed, <code class="docutils literal"><span class="pre">pegasus_analyzer</span></code> will provide you with a command to recieve more information about the failed jobs in the subdax.</p>
</div>
</div>
<div class="section" id="reuse-of-data-from-a-previous-workflow">
<span id="weeklyahopereuse"></span><h2>Reuse of data from a previous workflow<a class="headerlink" href="#reuse-of-data-from-a-previous-workflow" title="Permalink to this headline">¶</a></h2>
<p>One of the features of  Pegasus is to reuse the data products of prior runs.
This can be used to expand an analysis or recover a run with mistaken settings without
duplicating work.</p>
<div class="section" id="generate-the-full-workflow-you-want-to-do">
<h3>Generate the full workflow you want to do<a class="headerlink" href="#generate-the-full-workflow-you-want-to-do" title="Permalink to this headline">¶</a></h3>
<p>First generate the full workflow for the
run you would like to do as normal. Follow the instructions of this page from <span class="xref std std-ref">howtorunworkflow</span>,
but stop before planning the workflow with plan.sh in <a class="reference internal" href="#coincworkflowplan"><span>Planning and Submitting the Workflow</span></a>.</p>
</div>
<div class="section" id="select-the-files-you-want-to-reuse-from-the-prior-run">
<h3>Select the files you want to reuse from the prior run<a class="headerlink" href="#select-the-files-you-want-to-reuse-from-the-prior-run" title="Permalink to this headline">¶</a></h3>
<p>Locate the directory of the run that you would like to reuse. There is a file
called <code class="docutils literal"><span class="pre">output.map</span></code> in the directory that you specified with the
<code class="docutils literal"><span class="pre">--output</span></code> argument to <code class="docutils literal"><span class="pre">pycbc_make_coinc_search_workflow</span></code>. This file contains a
listing of all of the data products of the prior workflow, and can be used to tell
pegasus to skip regenerating them.</p>
<p>Select the entries in the file that you would like to skip generating again and
place that into a new file. The example below selects all the inspiral and
tmpltbank jobs and places their entries into a new listing called prior_data.map.:</p>
<div class="highlight-python"><div class="highlight"><pre># Lets get the tmpltbank entries
cat /path/to/old/run/${GPS_START_TIME}-${GPS_END_TIME}/output.map | grep &#39;TMPLTBANK&#39; &gt; prior_data.map

# Add in the inspiral  files
cat /path/to/old/run/${GPS_START_TIME}-${GPS_END_TIME}/output.map | grep &#39;INSPIRAL&#39; &gt;&gt; prior_data.map
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">You can include files in the prior data listing that wouldn&#8217;t be generated anyway by your new run. These are simply
ignored.</p>
</div>
</div>
<div class="section" id="plan-the-workflow">
<h3>Plan the workflow<a class="headerlink" href="#plan-the-workflow" title="Permalink to this headline">¶</a></h3>
<p>From the directory where the dax was created, now plan the workflow with an additional argument as follows.:</p>
<div class="highlight-python"><div class="highlight"><pre>pycbc_submit_dax --accounting-group ligo.dev.o1.cbc.explore.test --dax s6d_chunk3.dax --cache /path/to/prior_data.map
</pre></div>
</div>
<p>Follow the remaining <a class="reference internal" href="#coincworkflowplan"><span>Planning and Submitting the Workflow</span></a> instructions to submit your reduced
workflow.</p>
</div>
</div>
<div class="section" id="running-on-the-open-science-grid">
<span id="weeklyahopeosg"></span><h2>Running on the Open Science Grid<a class="headerlink" href="#running-on-the-open-science-grid" title="Permalink to this headline">¶</a></h2>
<div class="section" id="prerequisites">
<h3>Prerequisites<a class="headerlink" href="#prerequisites" title="Permalink to this headline">¶</a></h3>
<p>There are a number of requirements on the machine on which the workflow will be started:</p>
<ul class="simple">
<li>Pegasus version 4.5.3 or later.</li>
<li>An updated version of Java SSL proxies.  Replace <code class="docutils literal"><span class="pre">share/pegasus/java/ssl-proxies-2.1.0.jar</span></code> with the one from <a class="reference external" href="http://gaul.isi.edu/pub/ssl-proxies-2.1.1-SNAPSHOT.jar">http://gaul.isi.edu/pub/ssl-proxies-2.1.1-SNAPSHOT.jar</a></li>
<li>The bundled executables available on the submit machine</li>
<li>A gridftp server running on the submit machine</li>
<li>Condor configuration (beyond the scope of this document)</li>
</ul>
<p>For the following instructions set <code class="docutils literal"><span class="pre">SUBMIT_MACHINE</span></code> to be the full name of the submit machine and <code class="docutils literal"><span class="pre">EXECUTABLE_PATH</span></code> to be the full path to the directory containing the
bundles.  You will need to set:</p>
<div class="highlight-python"><div class="highlight"><pre>export PATH=${EXECUTABLE_PATH}:${PATH}
</pre></div>
</div>
<p>to ensure the correct versions are found by the workflow generator.</p>
</div>
<div class="section" id="generating-a-cache-file">
<h3>Generating a cache file<a class="headerlink" href="#generating-a-cache-file" title="Permalink to this headline">¶</a></h3>
<p>In order to get frame files, jobs running on OSG need a cache file that points to the gridFTP server at Nebraska.:</p>
<div class="highlight-python"><div class="highlight"><pre>find /frames/ER8/*HOFT_C00* /frames/O1/*HOFT_C00* -type f -name \*gwf &gt; cache.tmp

sed -e &#39;s+.*/\([^/]*\)$+\1+&#39; -e &#39;s+.*\([0-9]\{6\}\)[0-9]\{4\}.*+\1/\0+&#39; cache.tmp &gt; short.tmp
sed -e &#39;s+^+gsiftp://red-gridftp.unl.edu/user/ligo+&#39; -e &#39;s+$+ pool=&quot;osg&quot;+&#39; cache.tmp &gt; long.tmp

paste -d&#39; &#39; short.tmp long.tmp &gt; osg-frames-c00.cache

rm cache.tmp short.tmp long.tmp
</pre></div>
</div>
<p>This will need to be tweaked depending on the submission site.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">If the inspiral jobs require any files that are not generated by the workflow, such as gating files, entries
for these files must be added to the cache manually.</p>
</div>
</div>
<div class="section" id="configuring-the-workflow">
<h3>Configuring the workflow<a class="headerlink" href="#configuring-the-workflow" title="Permalink to this headline">¶</a></h3>
<p>In order for <code class="docutils literal"><span class="pre">pycbc_inspiral</span></code> to be sent to worker nodes it must be available
via a remote protocol, either http or gridFTP.  The easiest way to ensure this
is to include the <code class="docutils literal"><span class="pre">executables.ini</span></code> file assocated with the current release
in the list of ini files, for example:</p>
<div class="highlight-python"><div class="highlight"><pre>http://code.pycbc.phy.syr.edu/pycbc-software/v1.3.1/x86_64/composer_xe_2015.0.090/
</pre></div>
</div>
<p>Add the following to the list of <code class="docutils literal"><span class="pre">--config-overrides</span></code> when running <code class="docutils literal"><span class="pre">pycbc_make_coinc_search_workflow</span></code>:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="s">&#39;pegasus_profile-inspiral:hints|execution.site:osg&#39;</span> \
<span class="s">&#39;pegasus_profile-inspiral:condor|request_memory:1920M&#39;</span> \
<span class="s">&#39;workflow-main:staging-site:osg=osg-scratch&#39;</span> \
</pre></div>
</div>
<p>If a custom <code class="docutils literal"><span class="pre">executables.ini</span></code> is being used it will also be necessary to mark <code class="docutils literal"><span class="pre">pycbc-inspiral</span></code> as
uninstalled by also adding:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="s">&#39;pegasus_profile-inspiral:pycbc|installed:False&#39;</span> \
</pre></div>
</div>
<p>to the list of <code class="docutils literal"><span class="pre">--config-overrides</span></code></p>
</div>
<div class="section" id="running-the-workflow">
<h3>Running the workflow<a class="headerlink" href="#running-the-workflow" title="Permalink to this headline">¶</a></h3>
<p>Add the following arguments to <code class="docutils literal"><span class="pre">pycbc_submit_dax</span></code>:</p>
<div class="highlight-python"><div class="highlight"><pre>--execution-sites osg \
--append-pegasus-property &#39;pegasus.data.configuration=nonsharedfs&#39; \
--append-site-profile &#39;local:dagman|maxidle:5000&#39; \
--remote-staging-server `hostname -f` \
--cache osg-frames-c00.cache \
</pre></div>
</div>
<p><code class="docutils literal"><span class="pre">hostname</span> <span class="pre">-f</span></code> will give the correct value if there is a gsiftp server running on the submit machine.  If not, change this as needed.</p>
</div>
</div>
</div>


           </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="pycbc_make_sngl_workflow.html" class="btn btn-neutral float-right" title="pycbc_make_sngl_workflow: A single-ifo detchar workflow generator" accesskey="n">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="pycbc_make_psd_estimation_workflow.html" class="btn btn-neutral" title="pycbc_make_psd_estimation_workflow: A workflow generator for noise estimation" accesskey="p"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2014, Alexander Nitz.
      Last updated on Jan 04, 2016.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../',
            VERSION:'0.1',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true
        };
    </script>
      <script type="text/javascript" src="../_static/jquery.js"></script>
      <script type="text/javascript" src="../_static/underscore.js"></script>
      <script type="text/javascript" src="../_static/doctools.js"></script>

  

  
  
    <script type="text/javascript" src="../_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>